id,text,embedding,metadata
0,"Given the large collections of unstructured and semi-structured data available on the web, there is a crucial need to enable quick and efficient access to the knowledge content within them. Traditionally, the field of information extraction has focused on extracting such knowledge from unstructured text documents, such as job postings, scientific papers, news articles, and emails. However, the content on the web increasingly contains more varied types of data, including semi-structured web pages, tables that do not adhere to any schema, photographs, videos, and audio. Given a query by a user, the appropriate information may appear in any of these different modes, and thus there's a crucial need for methods to construct knowledge bases from different types of data, and more importantly, combine the evidence in order to extract the correct answer.",,{}
1,"Motivated by this goal, we introduce the task of multimodal attribute extraction. Provided contextual information about an entity, in the form of any of the modes described above, along with an attribute query, the goal is to extract the corresponding value for that attribute. While attribute extraction on the domain of text has been well-studied BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , to our knowledge this is the first time attribute extraction using a combination of multiple modes of data has been considered. This introduces additional challenges to the problem, since a multimodal attribute extractor needs to be able to return values provided any kind of evidence, whereas modern attribute extractors treat attribute extraction as a tagging problem and thus only work when attributes occur as a substring of text.",,{}
2,"In order to support research on this task, we release the Multimodal Attribute Extraction (MAE) dataset, a large dataset containing mixed-media data for over 2.2 million commercial product items, collected from a large number of e-commerce sites using the Diffbot Product API. The collection of items is diverse and includes categories such as electronic products, jewelry, clothing, vehicles, and real estate. For each item, we provide a textual product description, collection of images, and open-schema table of attribute-value pairs (see Figure 1 for an example). The provided attribute-value pairs only provide a very weak source of supervision; where the value might appear in the context is not known, and further, it is not even guaranteed that the value can be extracted from the provided evidence. In all, there are over 4 million images and 7.6 million attribute-value pairs.",,{}
3,"In order to support research on this task, we release the Multimodal Attribute Extraction (MAE) dataset, a large dataset containing mixed-media data for over 2.2 million commercial product items, collected from a large number of e-commerce sites using the Diffbot Product API. The collection of items is diverse and includes categories such as electronic products, jewelry, clothing, vehicles, and real estate. For each item, we provide a textual product description, collection of images, and open-schema table of attribute-value pairs (see Figure 1 for an example). The provided attribute-value pairs only provide a very weak source of supervision; where the value might appear in the context is not known, and further, it is not even guaranteed that the value can be extracted from the provided evidence. In all, there are over 4 million images and 7.6 million attribute-value pairs. By releasing such a large dataset, we hope to drive progress on this task similar to how the Penn Treebank BIBREF5 , SQuAD BIBREF6 , and Imagenet BIBREF7 have driven progress on syntactic parsing, question answering, and object recognition, respectively.",,{}
4,"To asses the difficulty of the task and the dataset, we first conduct a human evaluation study using Mechanical Turk that demonstrates that all available modes of information are useful for detecting values. We also train and provide results for a variety of machine learning models on the dataset. We observe that a simple most-common value classifier, which always predicts the most-common value for a given attribute, provides a very difficult baseline for more complicated models to beat (33% accuracy). In our current experiments, we are unable to train an image-only classifier that can outperform this simple model, despite using modern neural architectures such as VGG-16 BIBREF8 and Google's Inception-v3 BIBREF9 . However, we are able to obtain significantly better performance using a text-only classifier (59% accuracy). We hope to improve and obtain more accurate models in further research.",,{}
5,"Since a multimodal attribute extractor needs to be able to return values for attributes which occur in images as well as text, we cannot treat the problem as a labeling problem as is done in the existing approaches to attribute extraction. We instead define the problem as following: Given a product $i$ and a query attribute $a$ , we need to extract a corresponding value $v$ from the evidence provided for $i$ , namely, a textual description of it ( $D_i$ ) and a collection of images ( $I_i$ ). For example, in Figure 1 , we observe the image and the description of a product, and examples of some attributes and values of interest.",,{}
6,"Since a multimodal attribute extractor needs to be able to return values for attributes which occur in images as well as text, we cannot treat the problem as a labeling problem as is done in the existing approaches to attribute extraction. We instead define the problem as following: Given a product $i$ and a query attribute $a$ , we need to extract a corresponding value $v$ from the evidence provided for $i$ , namely, a textual description of it ( $D_i$ ) and a collection of images ( $I_i$ ). For example, in Figure 1 , we observe the image and the description of a product, and examples of some attributes and values of interest. For training, for a set of product items $\mathcal {I}$ , we are given, for each item $i \in \mathcal {I}$ , its textual description $D_i$ and the images $I_i$ , and a set $a$0 comprised of attribute-value pairs (i.e.",,{}
7,"We instead define the problem as following: Given a product $i$ and a query attribute $a$ , we need to extract a corresponding value $v$ from the evidence provided for $i$ , namely, a textual description of it ( $D_i$ ) and a collection of images ( $I_i$ ). For example, in Figure 1 , we observe the image and the description of a product, and examples of some attributes and values of interest. For training, for a set of product items $\mathcal {I}$ , we are given, for each item $i \in \mathcal {I}$ , its textual description $D_i$ and the images $I_i$ , and a set $a$0 comprised of attribute-value pairs (i.e. $a$1 ).",,{}
8,"We instead define the problem as following: Given a product $i$ and a query attribute $a$ , we need to extract a corresponding value $v$ from the evidence provided for $i$ , namely, a textual description of it ( $D_i$ ) and a collection of images ( $I_i$ ). For example, in Figure 1 , we observe the image and the description of a product, and examples of some attributes and values of interest. For training, for a set of product items $\mathcal {I}$ , we are given, for each item $i \in \mathcal {I}$ , its textual description $D_i$ and the images $I_i$ , and a set $a$0 comprised of attribute-value pairs (i.e. $a$1 ). In general, the products at query time will not be in $a$2 , and we do not assume any fixed ontology for products, attributes, or values.",,{}
9,"For example, in Figure 1 , we observe the image and the description of a product, and examples of some attributes and values of interest. For training, for a set of product items $\mathcal {I}$ , we are given, for each item $i \in \mathcal {I}$ , its textual description $D_i$ and the images $I_i$ , and a set $a$0 comprised of attribute-value pairs (i.e. $a$1 ). In general, the products at query time will not be in $a$2 , and we do not assume any fixed ontology for products, attributes, or values. We evaluate the performance on this task as the accuracy of the predicted value with the observed value, however since there may be multiple correct values, we also include hits@ $a$3 evaluation.",,{}
10,"In this section, we formulate a novel extraction model for the task that builds upon the architectures used recently in tasks such as image captioning, question answering, VQA, etc. The model is composed of three separate modules: (1) an encoding module that uses modern neural architectures to jointly embed the query, text, and images into a common latent space, (2) a fusion module that combines these embedded vectors using an attribute-specific attention mechanism to a single dense vector, and (3) a similarity-based value decoder which produces the final value prediction. We provide an overview of this architecture in Figure 3 .",,{}
11,"We evaluate on a subset of the MAE dataset consisting of the 100 most common attributes, covering roughly 50% of the examples in the overall MAE dataset. To determine the relative effectiveness of the different modes of information, we train image and text only versions of the model described above. Following the suggestions in BIBREF15 we use a 600 unit single layer in our text convolutions, and a 5 word window size. We apply dropout to the output of both the image and text CNNs before feeding the output through fully connected layers to obtain the image and text embeddings. Employing a coarse grid search, we found models performed best using a large embedding dimension of $k=1024$ . Lastly, we explore multimodal models using both the Concat and the GMU strategies. To evaluate models we use the hits@ $k$ metric on the values.",,{}
12,"The results of our experiments are summarized in Table 1 . We include a simple most-common value model that always predicts the most-common value for a given attribute. Observe that the performance of the image baseline model is almost identical to the most-common value model. Similarly, the performance of the multimodal models is similar to the text baseline model. Thus our models so far have been unable to effectively incorporate information from the image data. These results show that the task is sufficiently challenging that even a complex neural model cannot solve the task, and thus is a ripe area for future research.",,{}
13,"Model predictions for the example shown in Figure 1 are given in Table 2 , along with their similarity scores. Observe that the predictions made by the current image baseline model are almost identical to the most-common value model. This suggests that our current image baseline model is essentially ignoring all of the image related information and instead learning to predict common values.",,{}
14,"Our work is related to, and builds upon, a number of existing approaches.",,{}
15,"The introduction of large curated datasets has driven progress in many fields of machine learning. Notable examples include: The Penn Treebank BIBREF5 for syntactic parsing models, Imagenet BIBREF7 for object recognition, Flickr30k BIBREF16 and MS COCO BIBREF17 for image captioning, SQuAD BIBREF6 for question answering and VQA BIBREF18 for visual question answering. Despite the interest in related tasks, there is currently no publicly available dataset for attribute extraction, let alone multimodal attribute extraction. This creates a high barrier to entry as anyone interested in attribute extraction must go through the expensive and time-consuming process of acquiring a dataset. Furthermore, there is no way to compare the effectiveness of different techniques. Our dataset aims to address this concern.",,{}
16,"Recently, there has been renewed interest in multimodal machine learning problems. BIBREF19 demonstrate an effective image captioning system that uses a CNN to encode an image which is used as the input to an LSTM BIBREF20 decoder, producing the output caption. This encoder-decoder architecture forms the basis for successful approaches to other multimodal problems such as visual question answering BIBREF21 . Another body of work focuses on the problem of unifying information from different modes of information. BIBREF22 propose to concatenate together the output of a text-based distributional model (such as word2vec BIBREF23 ) with an encoding produced from a CNN applied to images of the word. BIBREF24 demonstrate an alternative approach to concatenation, where instead the a word embedding is learned that minimizes a joint loss function involving context-prediction and image reconstruction losses.",,{}
17,"Recently, there has been renewed interest in multimodal machine learning problems. BIBREF19 demonstrate an effective image captioning system that uses a CNN to encode an image which is used as the input to an LSTM BIBREF20 decoder, producing the output caption. This encoder-decoder architecture forms the basis for successful approaches to other multimodal problems such as visual question answering BIBREF21 . Another body of work focuses on the problem of unifying information from different modes of information. BIBREF22 propose to concatenate together the output of a text-based distributional model (such as word2vec BIBREF23 ) with an encoding produced from a CNN applied to images of the word. BIBREF24 demonstrate an alternative approach to concatenation, where instead the a word embedding is learned that minimizes a joint loss function involving context-prediction and image reconstruction losses. Another alternative to concatenation is the gated multimodal unit (GMU) proposed in BIBREF13 .",,{}
18,"BIBREF19 demonstrate an effective image captioning system that uses a CNN to encode an image which is used as the input to an LSTM BIBREF20 decoder, producing the output caption. This encoder-decoder architecture forms the basis for successful approaches to other multimodal problems such as visual question answering BIBREF21 . Another body of work focuses on the problem of unifying information from different modes of information. BIBREF22 propose to concatenate together the output of a text-based distributional model (such as word2vec BIBREF23 ) with an encoding produced from a CNN applied to images of the word. BIBREF24 demonstrate an alternative approach to concatenation, where instead the a word embedding is learned that minimizes a joint loss function involving context-prediction and image reconstruction losses. Another alternative to concatenation is the gated multimodal unit (GMU) proposed in BIBREF13 . We investigate the performance of different techniques for combining image and text data for product attribute extraction in section ""Experiments"" .",,{}
19,"To our knowledge, we are the first to study the problem of attribute extraction from multimodal data. However the problem of attribute extraction from text is well studied. BIBREF1 treat attribute extraction of retail products as a form of named entity recognition. They predefine a list of attributes to extract and train a Naïve Bayes model on a manually labeled seed dataset to extract the corresponding values. BIBREF3 build on this work by bootstrapping to expand the seed list, and evaluate more complicated models such as HMMs, MaxEnt, SVMs, and CRFs. To mitigate the introduction noisy labels when using semi-supervised techniques, BIBREF2 incorporates crowdsourcing to manually accept or reject the newly introduced labels. One major drawback of these approaches is that they require manually labelled seed data to construct the knowledge base of attribute-value pairs, which can be quite expensive for a large number of attributes.",,{}
20,"To our knowledge, we are the first to study the problem of attribute extraction from multimodal data. However the problem of attribute extraction from text is well studied. BIBREF1 treat attribute extraction of retail products as a form of named entity recognition. They predefine a list of attributes to extract and train a Naïve Bayes model on a manually labeled seed dataset to extract the corresponding values. BIBREF3 build on this work by bootstrapping to expand the seed list, and evaluate more complicated models such as HMMs, MaxEnt, SVMs, and CRFs. To mitigate the introduction noisy labels when using semi-supervised techniques, BIBREF2 incorporates crowdsourcing to manually accept or reject the newly introduced labels. One major drawback of these approaches is that they require manually labelled seed data to construct the knowledge base of attribute-value pairs, which can be quite expensive for a large number of attributes. BIBREF0 address this problem by using an unsupervised, LDA-based approach to generate word classes from reviews, followed by aligning them to the product description.",,{}
21,"BIBREF1 treat attribute extraction of retail products as a form of named entity recognition. They predefine a list of attributes to extract and train a Naïve Bayes model on a manually labeled seed dataset to extract the corresponding values. BIBREF3 build on this work by bootstrapping to expand the seed list, and evaluate more complicated models such as HMMs, MaxEnt, SVMs, and CRFs. To mitigate the introduction noisy labels when using semi-supervised techniques, BIBREF2 incorporates crowdsourcing to manually accept or reject the newly introduced labels. One major drawback of these approaches is that they require manually labelled seed data to construct the knowledge base of attribute-value pairs, which can be quite expensive for a large number of attributes. BIBREF0 address this problem by using an unsupervised, LDA-based approach to generate word classes from reviews, followed by aligning them to the product description. BIBREF4 propose to extract attribute-value pairs from structured data on product pages, such as HTML tables, and lists, to construct the KB.",,{}
22,"BIBREF3 build on this work by bootstrapping to expand the seed list, and evaluate more complicated models such as HMMs, MaxEnt, SVMs, and CRFs. To mitigate the introduction noisy labels when using semi-supervised techniques, BIBREF2 incorporates crowdsourcing to manually accept or reject the newly introduced labels. One major drawback of these approaches is that they require manually labelled seed data to construct the knowledge base of attribute-value pairs, which can be quite expensive for a large number of attributes. BIBREF0 address this problem by using an unsupervised, LDA-based approach to generate word classes from reviews, followed by aligning them to the product description. BIBREF4 propose to extract attribute-value pairs from structured data on product pages, such as HTML tables, and lists, to construct the KB. This is essentially the approach used to construct the knowledge base of attribute-value pairs used in our work, which is automatically performed by Diffbot's Product API.",,{}
23,"In order to kick start research on multimodal information extraction problems, we introduce the multimodal attribute extraction dataset, an attribute extraction dataset derived from a large number of e-commerce websites. MAE features images, textual descriptions, and attribute-value pairs for a diverse set of products. Preliminary data from an Amazon Mechanical Turk study demonstrates that both modes of information are beneficial to attribute extraction. We measure the performance of a collection of baseline models, and observe that reasonably high accuracy can be obtained using only text. However, we are unable to train off-the-shelf methods to effectively leverage image data.",,{}
24,"There are a number of exciting avenues for future research. We are interested in performing a more comprehensive crowdsourcing study to identify the ways in which different evidence forms are useful, and in order to create clean evaluation data. As this dataset brings up interesting challenges in multimodal machine learning, we will explore a variety of novel architectures that are able to combine the different forms of evidence effectively to accurately extract the attribute values. Finally, we are also interested in exploring other aspects of knowledge base construction that may benefit from multimodal reasoning, such as relational prediction, entity linking, and disambiguation.",,{}
